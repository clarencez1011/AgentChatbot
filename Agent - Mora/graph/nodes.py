from schemas import AgentState
from config import settings
from services.llm import llm_service
from services.vector import vec_service
from services.search import search_service

# --- ç§»æ¤ä½ çš„ Prompts ç±» ---
class Prompts:
    SYSTEM_REWRITE = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ IT æœç´¢ä¼˜åŒ–ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä¼˜åŒ–ç”¨æˆ·çš„è¾“å…¥ï¼Œä»¥ä¾¿åœ¨ IT çŸ¥è¯†åº“ä¸­è¿›è¡Œæ£€ç´¢ã€‚
    è§„åˆ™ï¼š
    1. å»é™¤å£è¯­åŒ–è¯æ±‡ï¼ˆå¦‚â€œé‚£ä¸ªâ€ã€â€œè¯·é—®â€ã€â€œæ•‘å‘½å•Šâ€ï¼‰ã€‚
    2. æå–æ ¸å¿ƒå…³é”®è¯ï¼Œè¡¥å……éšå«çš„ä¸»è¯­ï¼ˆå¦‚å°†â€œè¿ä¸ä¸Šâ€æ”¹ä¸ºâ€œVPNè¿æ¥å¤±è´¥â€ï¼‰ã€‚
    3. è½¬åŒ–ä¸ºç®€ç»ƒã€ä¸“ä¸šçš„æœç´¢çŸ­è¯­ã€‚
    4. ã€é‡è¦ã€‘å¦‚æœç”¨æˆ·æ˜¯åœ¨é—²èŠï¼ˆå¦‚â€œä½ å¥½â€ã€â€œè°¢è°¢â€ï¼‰ï¼Œè¯·åŸå°ä¸åŠ¨åœ°è¿”å›åŸæ–‡ï¼Œä¸è¦ä¿®æ”¹ã€‚
    ç›´æ¥è¾“å‡ºä¼˜åŒ–åçš„æ–‡æœ¬ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šã€‚"""

    SYSTEM_ROUTER = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ„å›¾åˆ†ç±»åŠ©æ‰‹ã€‚å¼ºåˆ¶è¿”å› JSONã€‚
    åˆ†ç±»æ ‡å‡†ï¼š
    1. "rag": IT æ•…éšœã€è½¯ä»¶æŠ¥é”™ã€è´¦å·é—®é¢˜ã€è®¾å¤‡é—®é¢˜ç­‰ä¸šåŠ¡é—®é¢˜ã€‚
    2. "chat": é—²èŠã€é—®å€™ã€æ— å…³è¯é¢˜ã€‚
    è¾“å‡ºæ ¼å¼ï¼š{"type": "rag", "reason": "..."}"""

    SYSTEM_RAG = """ä½ æ˜¯ç›æ°ä¸­å›½ IT æ”¯æŒåŠ©æ‰‹ã€‚ä½ å¿…é¡»ä¸¥æ ¼åŸºäºã€çŸ¥è¯†åº“ã€‘å›ç­”ã€‚"""
    USER_RAG = """ã€çŸ¥è¯†åº“ã€‘\n{context}\n\nã€ç”¨æˆ·é—®é¢˜ã€‘\n{question}\n\nã€ä»»åŠ¡ã€‘\nä»…ä½¿ç”¨çŸ¥è¯†åº“æ­¥éª¤å›ç­”ã€‚"""

    SYSTEM_SEARCH = """ä½ æ˜¯ç›æ°ä¸­å›½ IT æ”¯æŒåŠ©æ‰‹ã€‚å½“å‰å†…éƒ¨çŸ¥è¯†åº“æ²¡æœ‰ç›¸å…³è®°å½•ï¼Œä½ æ­£åœ¨å‚è€ƒå¤–éƒ¨äº’è”ç½‘æœç´¢ç»“æœæ¥è¾…åŠ©ç”¨æˆ·ã€‚
    è¯·æ•´åˆæœç´¢ç»“æœï¼Œç»™å‡ºä¸€ä¸ªæ¸…æ™°ã€æœ‰æ¡ç†çš„è§£å†³æ–¹æ¡ˆã€‚
    æ³¨æ„ï¼šå¿…é¡»åœ¨å›ç­”å¼€å¤´æ˜ç¡®æ ‡æ³¨ï¼šâ€œâš ï¸ å†…éƒ¨çŸ¥è¯†åº“æœªæ‰¾åˆ°è®°å½•ï¼Œä»¥ä¸‹æ˜¯åŸºäºäº’è”ç½‘æœç´¢çš„å»ºè®®ï¼Œä»…ä¾›å‚è€ƒï¼šâ€"""
    USER_SEARCH = """ã€æœç´¢ç»“æœã€‘\n{context}\n\nã€ç”¨æˆ·é—®é¢˜ã€‘\n{question}"""

    SYSTEM_CHAT = """ä½ æ˜¯ç›æ°ä¸­å›½ IT æ”¯æŒåŠ©æ‰‹ã€‚ç”¨æˆ·åœ¨é—²èŠï¼Œè¯·ç®€çŸ­å‹å¥½å›åº”ã€‚"""

    SYSTEM_GRADER = """ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„é˜…å·è€å¸ˆã€‚è¯„ä¼° AI ç”Ÿæˆçš„ç­”æ¡ˆæ˜¯å¦å‡ºç°äº†â€œå¹»è§‰â€ã€‚
    å¿…é¡»è¿”å› JSONï¼š{"score": "yes", "reason": "..."} æˆ– {"score": "no", "reason": "..."}"""

# --- Nodes å®ç° (Async) ---

async def node_rewrite(state: AgentState):
    raw_q = state["question"]
    orig_q = state.get("original_question") or raw_q
    print(f"   [Rewrite] Original: {orig_q}")
    
    new_q = await llm_service.rewrite_query(orig_q, Prompts.SYSTEM_REWRITE)
    print(f"   [Rewrite] Optimized: {new_q}")
    
    return {"question": new_q, "original_question": orig_q}

async def node_router(state: AgentState):
    q = state["question"]
    decision = await llm_service.route_request(q, Prompts.SYSTEM_ROUTER)
    route_type = decision.get("type", "rag")
    print(f"   [Router] Decision: {route_type.upper()}")

    dense_vec = None
    if route_type == "rag":
        # å¼‚æ­¥è°ƒç”¨ embedding
        dense_vec = await vec_service.embed_query_async(q)
        
    return {"route": route_type, "dense_vec": dense_vec}

async def node_retriever(state: AgentState):
    docs = await vec_service.hybrid_search_async(
        state["question"], 
        state["dense_vec"], 
        top_k=settings.TOP_K
    )
    return {"documents": docs}

async def node_gate(state: AgentState):
    matches = state["documents"]
    
    if not matches:
        print("   [Gate] âŒ No documents retrieved.")
        return {"retrieval_quality": False}
    
    s1 = float(matches[0].get("score", 0.0))
    s2 = float(matches[1].get("score", 0.0)) if len(matches) > 1 else 0.0
    
    print(f"   [Gate] Top1: {s1:.4f} | Top2: {s2:.4f} | Margin: {s1-s2:.4f}")

    # é€»è¾‘ä¿æŒå®Œå…¨ä¸€è‡´
    if s1 < settings.SCORE_FLOOR:
        print(f"   [Gate] ğŸ“‰ Low Score ({s1:.4f} < {settings.SCORE_FLOOR}) -> Search")
        return {"retrieval_quality": False}
    
    if s1 >= settings.HIGH_CONFIDENCE:
        print(f"   [Gate] ğŸš€ High Confidence ({s1:.4f} >= {settings.HIGH_CONFIDENCE}) -> PASS")
        return {"retrieval_quality": True}

    if (s1 - s2) < settings.MARGIN_FLOOR:
        print(f"   [Gate] âš ï¸ Ambiguous in Mid-Range -> Search")
        return {"retrieval_quality": False}
    
    print(f"   [Gate] âœ… Quality Check Passed")
    return {"retrieval_quality": True}

async def node_generate_rag(state: AgentState):
    blocks = [f"ã€æ•…éšœåœºæ™¯ã€‘{m['metadata'].get('name')}\nã€å¤„ç†æ­¥éª¤ã€‘{m['metadata'].get('steps')}" for m in state["documents"]]
    context = "\n\n".join(blocks)
    
    user_p = Prompts.USER_RAG.format(context=context, question=state["question"])
    ans = await llm_service.generate(Prompts.SYSTEM_RAG, user_p)
    return {"generation": ans}

async def node_grader(state: AgentState):
    print("   [Grader] Checking for hallucinations...")
    answer = state["generation"]
    blocks = [m.get("metadata", {}).get("steps", "") for m in state["documents"]]
    context = "\n".join(blocks)
    
    user_content = f"ã€å‚è€ƒæ–‡æ¡£ã€‘\n{context}\n\nã€ç”Ÿæˆç­”æ¡ˆã€‘\n{answer}"
    grade = await llm_service.route_request(user_content, Prompts.SYSTEM_GRADER)
    score = grade.get("score", "yes")
    reason = grade.get("reason", "")
    
    if score == "yes":
        print(f"   [Grader] âœ… Approved. Reason: {reason}")
        return {"grade_status": "useful"}
    else:
        print(f"   [Grader] âŒ Hallucination detected. Reason: {reason}")
        print("   [Grader] ğŸ”„ Falling back to Web Search...")
        return {"grade_status": "not_useful"}

async def node_web_search(state: AgentState):
    print(f"   [Search] Searching Tavily for: {state['question']}...")
    res = await search_service.web_search_async(state["question"])
    return {"search_context": res}

async def node_generate_search(state: AgentState):
    user_p = Prompts.USER_SEARCH.format(context=state["search_context"], question=state["question"])
    ans = await llm_service.generate(Prompts.SYSTEM_SEARCH, user_p)
    return {"generation": ans}

async def node_generate_chat(state: AgentState):
    ans = await llm_service.generate(Prompts.SYSTEM_CHAT, state["question"], temp=0.5)
    return {"generation": ans}